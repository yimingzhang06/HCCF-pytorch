{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Params.py"
      ],
      "metadata": {
        "id": "K9Dar0OtEOGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser('Model Description')\n",
        "  parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
        "  parser.add_argument('--batch', default=256, type=int, help='batch size')\n",
        "  parser.add_argument('--sslbatch', default=4096, type=int, help='SSL batch size')\n",
        "  parser.add_argument('--reg', default=1e-5, type=float, help='weight decay regularizer')\n",
        "  parser.add_argument('--epoch', default=100, type=int, help='number of epochs')\n",
        "  parser.add_argument('--decayRate', default=0.96, type=float, help='decay rate for learning rate')\n",
        "  parser.add_argument('--save_path', default='tem', help='file name to save model and training record')\n",
        "  parser.add_argument('--latdim', default=32, type=int, help='embedding size')\n",
        "  parser.add_argument('--rank', default=4, type=int, help='embedding size')\n",
        "  parser.add_argument('--memosize', default=2, type=int, help='memory size')\n",
        "  parser.add_argument('--n_factors', default=4, type=int, help='Number of factors to disentangle the original embed-size representation.')\n",
        "  parser.add_argument('--n_iterations', default=2, type=int, help='Number of iterations to perform the routing mechanism.')\n",
        "  parser.add_argument('--sampNum', default=40, type=int, help='batch size for sampling')\n",
        "  parser.add_argument('--att_head', default=2, type=int, help='number of attention heads')\n",
        "  parser.add_argument('--gnn_layer', default=2, type=int, help='number of gnn layers')\n",
        "  parser.add_argument('--hyperNum', default=128, type=int, help='number of hyper edges')\n",
        "  parser.add_argument('--trnNum', default=10000, type=int, help='number of training instances per epoch')\n",
        "  parser.add_argument('--load_model', default=None, help='model name to load')\n",
        "  parser.add_argument('--shoot', default=20, type=int, help='K of top k')\n",
        "  parser.add_argument('--data', default='yelp', type=str, help='name of dataset')\n",
        "  parser.add_argument('--target', default='buy', type=str, help='target behavior to predict on')\n",
        "  parser.add_argument('--deep_layer', default=0, type=int, help='number of deep layers to make the final prediction')\n",
        "  parser.add_argument('--mult', default=100, type=float, help='multiplier for the result')\n",
        "  #parser.add_argument('--keepRate', default=0.5, type=float, help='rate for dropout')\n",
        "  parser.add_argument('--droprate', default=0.5, type=float, help='rate for dropout')\n",
        "  parser.add_argument('--slot', default=5, type=float, help='length of time slots')\n",
        "  parser.add_argument('--graphSampleN', default=15000, type=int, help='use 25000 for training and 200000 for testing, empirically')\n",
        "  parser.add_argument('--divSize', default=10000, type=int, help='div size for smallTestEpoch')\n",
        "  parser.add_argument('--tstEpoch', default=3, type=int, help='number of epoch to test while training')\n",
        "  parser.add_argument('--subUsrSize', default=10, type=int, help='number of item for each sub-user')\n",
        "  parser.add_argument('--subUsrDcy', default=0.9, type=float, help='decay factor for sub-users over time')\n",
        "  parser.add_argument('--leaky', default=0.5, type=float, help='slope for leaky relu')\n",
        "  parser.add_argument('--hyperReg', default=1e-4, type=float, help='regularizer for hyper connections')\n",
        "  parser.add_argument('--temp', default=1, type=float, help='temperature in ssl loss')\n",
        "  parser.add_argument('--ssl_reg', default=1e-4, type=float, help='reg weight for ssl loss')\n",
        "  parser.add_argument('--percent', default=0.0, type=float, help='percent of noise for noise robust test')\n",
        "  parser.add_argument('--tstNum', default=99, type=int, help='Numer of negative samples while testing, -1 for all negatives')\n",
        "  parser.add_argument('--seed', default=10, type=int, help='Random seed')\n",
        "\n",
        "  #\treturn parser.parse_args()\n",
        "  return parser\n",
        "\n",
        "args, _ = parse_args().parse_known_args()\n",
        "args.decay_step = args.trnNum/args.batch\n",
        "if torch.cuda.is_available():\n",
        "\targs.device = \"cuda\"\n",
        "else:\n",
        "\targs.device = \"cpu\"\n"
      ],
      "metadata": {
        "id": "6aqzIwtwEFQK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataHandler.py"
      ],
      "metadata": {
        "id": "8NSiLcVCEIsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "#from Params import args\n",
        "\n",
        "\n",
        "class DataHandler:\n",
        "  def __init__(self):\n",
        "    if args.data == 'yelp':\n",
        "        predir = '/content/drive/MyDrive/HCCF_data/yelp/'\n",
        "    elif args.data == 'ml10m':\n",
        "        predir = '/content/drive/MyDrive/HCCF_data/ml10m'\n",
        "    elif args.data == 'amazon':\n",
        "        predir = '/content/drive/MyDrive/HCCF_data/amazon/'\n",
        "    self.predir = predir\n",
        "    self.trnfile = predir + 'trnMat.pkl'\n",
        "    self.tstfile = predir + 'tstMat.pkl'\n",
        "\n",
        "  def LoadData(self):\n",
        "    if args.percent > 1e-8:\n",
        "      print('noised')\n",
        "      with open(self.predir + 'noise_%.2f' % args.percent, 'rb') as fs:\n",
        "        trnMat = (pickle.load(fs) != 0).astype(np.float32)\n",
        "    else:\n",
        "      with open(self.trnfile, 'rb') as fs:\n",
        "        trnMat = (pickle.load(fs) != 0).astype(np.float32)\n",
        "    # test set\n",
        "    with open(self.tstfile, 'rb') as fs:\n",
        "      tstMat = pickle.load(fs)\n",
        "      # tstMat = (pickle.load(fs) != 0).astype(np.float32)\n",
        "    tstLocs = [None] * tstMat.shape[0]\n",
        "    tstUsrs = set()\n",
        "    for i in range(len(tstMat.data)):\n",
        "      row = tstMat.row[i]\n",
        "      col = tstMat.col[i]\n",
        "      if tstLocs[row] is None:\n",
        "        tstLocs[row] = list()\n",
        "      tstLocs[row].append(col)\n",
        "      tstUsrs.add(row)\n",
        "    tstUsrs = np.array(list(tstUsrs))\n",
        "\n",
        "    self.trnMat = trnMat\n",
        "    self.tstLocs = tstLocs\n",
        "    self.tstUsrs = tstUsrs\n",
        "    args.user, args.item = self.trnMat.shape\n",
        "    self.prepareGlobalData()\n",
        "\n",
        "  def prepareGlobalData(self):\n",
        "    adj = self.trnMat\n",
        "    adj = (adj != 0).astype(np.float32)\n",
        "    self.labelP = np.squeeze(np.array(np.sum(adj, axis=0)))\n",
        "    tpadj = transpose(adj)\n",
        "    adjNorm = np.reshape(np.array(np.sum(adj, axis=1)), [-1])\n",
        "    tpadjNorm = np.reshape(np.array(np.sum(tpadj, axis=1)), [-1])\n",
        "    for i in range(adj.shape[0]):\n",
        "      for j in range(adj.indptr[i], adj.indptr[i+1]):\n",
        "        adj.data[j] /= adjNorm[i]\n",
        "    for i in range(tpadj.shape[0]):\n",
        "      for j in range(tpadj.indptr[i], tpadj.indptr[i+1]):\n",
        "        tpadj.data[j] /= tpadjNorm[i]\n",
        "    self.adj = adj\n",
        "    self.tpadj = tpadj\n",
        "\n",
        "def transpose(mat):\n",
        "\tcoomat = coo_matrix(mat)\n",
        "\treturn csr_matrix(coomat.transpose())\n",
        "\n",
        "def negSamp(temLabel, sampSize, nodeNum):\n",
        "\tnegset = [None] * sampSize\n",
        "\tcur = 0\n",
        "\twhile cur < sampSize:\n",
        "\t\trdmItm = np.random.choice(nodeNum)\n",
        "\t\tif temLabel[rdmItm] == 0:\n",
        "\t\t\tnegset[cur] = rdmItm\n",
        "\t\t\tcur += 1\n",
        "\treturn negset\n",
        "\n",
        "def transToLsts(mat, mask=False, norm=False):\n",
        "  shape = torch.Size(mat.shape)\n",
        "  mat = sp.coo_matrix(mat)\n",
        "  indices = torch.from_numpy(np.vstack((mat.row, mat.col)).astype(np.int64))\n",
        "  data = mat.data\n",
        "  \n",
        "  if norm:\n",
        "    rowD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=1) + 1e-8) + 1e-8)))\n",
        "    colD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=0) + 1e-8) + 1e-8)))\n",
        "    for i in range(len(mat.data)):\n",
        "      row = indices[0, i]\n",
        "      col = indices[1, i]\n",
        "      data[i] = data[i] * rowD[row] * colD[col]\n",
        "\t# half mask\n",
        "  if mask:\n",
        "    spMask = (np.random.uniform(size=data.shape) > 0.5) * 1.0\n",
        "    data = data * spMask\n",
        "\n",
        "  if indices.shape[0] == 0:\n",
        "    indices = np.array([[0, 0]], dtype=np.int32)\n",
        "    data = np.array([0.0], np.float32)\n",
        "\n",
        "  data = torch.from_numpy(data)\n",
        "\t#a =torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()\n",
        "  return indices, data, shape\n"
      ],
      "metadata": {
        "id": "KZnHP9bXEFEv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TimeLogger.py"
      ],
      "metadata": {
        "id": "Xpn_Xh6tEPc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "logmsg = ''\n",
        "timemark = dict()\n",
        "saveDefault = False\n",
        "def log(msg, save=None, oneline=False):\n",
        "\tglobal logmsg\n",
        "\tglobal saveDefault\n",
        "\ttime = datetime.datetime.now()\n",
        "\ttem = '%s: %s' % (time, msg)\n",
        "\tif save != None:\n",
        "\t\tif save:\n",
        "\t\t\tlogmsg += tem + '\\n'\n",
        "\telif saveDefault:\n",
        "\t\tlogmsg += tem + '\\n'\n",
        "\tif oneline:\t\n",
        "\t\tprint(tem, end='\\r')\n",
        "\telse:\n",
        "\t\tprint(tem)\n",
        "\n",
        "def marktime(marker):\n",
        "\tglobal timemark\n",
        "\ttimemark[marker] = datetime.datetime.now()\n",
        "\n",
        "def SpentTime(marker):\n",
        "\tglobal timemark\n",
        "\tif marker not in timemark:\n",
        "\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n",
        "\t\ttem = '%s: %s' % (time, msg)\n",
        "\t\tprint(tem)\n",
        "\t\treturn False\n",
        "\treturn datetime.datetime.now() - timemark[marker]\n",
        "\n",
        "def SpentTooLong(marker, day=0, hour=0, minute=0, second=0):\n",
        "\tglobal timemark\n",
        "\tif marker not in timemark:\n",
        "\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n",
        "\t\ttem = '%s: %s' % (time, msg)\n",
        "\t\tprint(tem)\n",
        "\t\treturn False\n",
        "\treturn datetime.datetime.now() - timemark[marker] >= datetime.timedelta(days=day, hours=hour, minutes=minute, seconds=second)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tlog('')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es8Zt-WcEE8Z",
        "outputId": "63491bde-25a0-4f19-8f48-4d07e71d07c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-07 13:10:28.828276: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model.py"
      ],
      "metadata": {
        "id": "f10X_nnlESis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from Params import args\n",
        "\n",
        "torch.manual_seed(666)\n",
        "\n",
        "def LeakyRelu(data):\n",
        "  #global leaky\n",
        "  ret = torch.maximum(args.leaky*data, data)\n",
        "  return ret\n",
        "\n",
        "class FC(nn.Module):\n",
        "  def __init__(self, inputDim, outDim, Bias = False, actFunc = None):\n",
        "    super(FC,self).__init__()\n",
        "    initializer = nn.init.xavier_normal_\n",
        "    self.W_fc = nn.Parameter(initializer(torch.empty(inputDim, outDim).cuda()))\n",
        "\n",
        "  def forward(self, inp, droprate = 0):\n",
        "    #W = self.W_fc.weight\n",
        "    fc1 = inp @ self.W_fc\n",
        "    ret = fc1\n",
        "    ret = LeakyRelu(ret)\n",
        "    return ret\n",
        "\n",
        "class hyperPropagate(nn.Module):\n",
        "  def __init__(self,inputdim):\n",
        "    super(hyperPropagate, self).__init__()\n",
        "    self.inputdim = inputdim\n",
        "    self.fc1 = FC(self.inputdim,args.hyperNum,actFunc = 'leakyRelu').cuda()\n",
        "    self.fc2 = FC(self.inputdim,args.hyperNum,actFunc = 'leakyRelu').cuda()\n",
        "    self.fc3 = FC(self.inputdim,args.hyperNum,actFunc = 'leakyRelu').cuda()\n",
        "    #self.actFunc = nn.LeakyReLU(negative_slope=args.leaky) \n",
        "\n",
        "  def forward(self,lats,adj):\n",
        "    lat1 = LeakyRelu(torch.transpose(adj,0,1) @ lats) #shape adj:user,hyperNum lats:user,latdim lat1:hypernum,latdim\n",
        "    lat2 = torch.transpose(self.fc1(torch.transpose(lat1,0,1)),0,1) + lat1 #shape hypernum,latdim\n",
        "    lat3 = torch.transpose(self.fc2(torch.transpose(lat2,0,1)),0,1) + lat2\n",
        "    lat4 = torch.transpose(self.fc3(torch.transpose(lat3,0,1)),0,1) + lat3\n",
        "    ret = adj @ lat4\n",
        "    ret = LeakyRelu(ret)\n",
        "    return ret\n",
        "\n",
        "class weight_trans(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(weight_trans, self).__init__()\n",
        "    initializer = nn.init.xavier_normal_\n",
        "    self.W = nn.Parameter(initializer(torch.empty(args.latdim, args.latdim).cuda()))\n",
        "\n",
        "  def forward(self,normalize):\n",
        "    ret = normalize @ self.W\n",
        "    return ret\n",
        "\n",
        "class HCCF(nn.Module):\n",
        "  def __init__(self, adj_py, tpAdj_py):\n",
        "    super(HCCF, self).__init__()\n",
        "    initializer = nn.init.xavier_normal_\n",
        "    self.uEmbed0 = nn.Parameter(initializer(torch.empty(args.user, args.latdim).cuda()))\n",
        "    self.iEmbed0 = nn.Parameter(initializer(torch.empty(args.item, args.latdim).cuda()))\n",
        "    self.uhyper = nn.Parameter(initializer(torch.empty(args.latdim, args.hyperNum).cuda()))\n",
        "    self.ihyper = nn.Parameter(initializer(torch.empty(args.latdim, args.hyperNum).cuda()))\n",
        "\n",
        "    self.adj = adj_py.cuda()#shape user,item\n",
        "    self.tpadj = tpAdj_py.cuda()#shape item,user\n",
        "\n",
        "    self.hyperULat_layers = nn.ModuleList()\n",
        "    self.hyperILat_layers = nn.ModuleList()\n",
        "    self.weight_layers = nn.ModuleList()\n",
        "    \n",
        "    for i in range(args.gnn_layer):\n",
        "      self.hyperULat_layers.append(hyperPropagate(args.hyperNum)) #shape hyperNum,hyperNum\n",
        "      self.hyperILat_layers.append(hyperPropagate(args.hyperNum)) #shape hyperNum,hyperNum\n",
        "      self.weight_layers.append(weight_trans())\n",
        "\n",
        "\n",
        "  def messagePropagate(self, lats, adj):\n",
        "    return LeakyRelu(torch.sparse.mm(adj, lats))\n",
        "\n",
        "  def calcSSL(self, hyperLat, gnnLat):\n",
        "    posScore = torch.exp(torch.sum(hyperLat * gnnLat, dim = 1) / args.temp)\n",
        "    negScore = torch.sum(torch.exp(gnnLat @ torch.transpose(hyperLat, 0, 1) / args.temp), dim = 1)\n",
        "    uLoss = torch.sum(-torch.log(posScore / (negScore + 1e-8) + 1e-8))\n",
        "    return uLoss\n",
        "\n",
        "  def Regularize(self, reg, method = 'L2'):\n",
        "    ret = 0.0\n",
        "    for i in range(len(reg)):\n",
        "        ret += torch.sum(torch.square(reg[i]))\n",
        "    return ret\n",
        "\n",
        "  def edgeDropout(self, mat, drop):\n",
        "    def dropOneMat(mat):\n",
        "      indices = mat._indices().cpu()\n",
        "      values = mat._values().cpu()\n",
        "      shape = mat.shape\n",
        "      newVals = nn.functional.dropout(values, p = drop)\n",
        "      return torch.sparse.FloatTensor(indices, newVals, shape).to(torch.float32).cuda()\n",
        "    return dropOneMat(mat)\n",
        "\n",
        "  def forward_test(self):\n",
        "    uEmbed0 = self.uEmbed0\n",
        "    iEmbed0 = self.iEmbed0\n",
        "    uhyper = self.uhyper\n",
        "    ihyper = self.ihyper\n",
        "\n",
        "    uuHyper = uEmbed0 @ uhyper#shape user,hyperNum\n",
        "    iiHyper = iEmbed0 @ ihyper#shape item,hyperNum\n",
        "\n",
        "    ulats = [uEmbed0]\n",
        "    ilats = [iEmbed0]\n",
        "\n",
        "    for i in range(args.gnn_layer):\n",
        "      ulat = self.messagePropagate(ilats[-1], self.edgeDropout(self.adj, drop = 0))\n",
        "      ilat = self.messagePropagate(ulats[-1], self.edgeDropout(self.tpadj, drop = 0))\n",
        "      hyperULat = self.hyperULat_layers[i](ulats[-1],nn.functional.dropout(uuHyper, p = 0))\n",
        "      hyperILat = self.hyperILat_layers[i](ilats[-1],nn.functional.dropout(iiHyper, p = 0))\n",
        "\n",
        "      ulats.append(ulat + hyperULat + ulats[-1])\n",
        "      ilats.append(ilat + hyperILat + ilats[-1])\n",
        "\n",
        "    ulat = sum(ulats)\n",
        "    ilat = sum(ilats)\n",
        "    return ulat, ilat\n",
        "\n",
        "  def forward(self, uids, iids, droprate = args.droprate):\n",
        "    uEmbed0 = self.uEmbed0\n",
        "    iEmbed0 = self.iEmbed0\n",
        "    uhyper = self.uhyper\n",
        "    ihyper = self.ihyper\n",
        "    gnnULats = []\n",
        "    gnnILats = []\n",
        "    hyperULats = []\n",
        "    hyperILats = []\n",
        "\n",
        "    ulats = [uEmbed0]\n",
        "    ilats = [iEmbed0]\n",
        "    for i in range(args.gnn_layer):\n",
        "      ulat = self.messagePropagate(ilats[-1], self.edgeDropout(self.adj, drop = droprate))\n",
        "      ilat = self.messagePropagate(ulats[-1], self.edgeDropout(self.tpadj, drop = droprate))\n",
        "      hyperULat = self.hyperULat_layers[i](ulats[-1],nn.functional.dropout(uEmbed0 @ uhyper, p = droprate))# / (1 - droprate))\n",
        "      hyperILat = self.hyperILat_layers[i](ilats[-1],nn.functional.dropout(iEmbed0 @ ihyper, p = droprate))#/ (1 - droprate) )\n",
        "\n",
        "      gnnULats.append(ulat)\n",
        "      gnnILats.append(ilat)\n",
        "      hyperULats.append(hyperULat)\n",
        "      hyperILats.append(hyperILat)\n",
        "\n",
        "      ulats.append(ulat + hyperULat + ulats[-1])\n",
        "      ilats.append(ilat + hyperILat + ilats[-1])\n",
        "\n",
        "    ulat = sum(ulats)\n",
        "    ilat = sum(ilats)\n",
        "    pckUlat = torch.index_select(ulat, 0, uids)\n",
        "    pckIlat = torch.index_select(ilat, 0, iids)\n",
        "    preds = torch.sum(pckUlat * pckIlat, dim=-1)\n",
        "\n",
        "    sslloss = 0\n",
        "    uniqUids = torch.unique(uids)\n",
        "    uniqIids = torch.unique(iids)\n",
        "\n",
        "    for i in range(len(hyperULats)):\n",
        "      pckHyperULat = self.weight_layers[i](torch.nn.functional.normalize(torch.index_select(hyperULats[i], 0, uniqUids), p=2, dim=1))# @ self.weight_layers[i].weight\n",
        "      pckGnnULat = torch.nn.functional.normalize(torch.index_select(gnnULats[i], 0, uniqUids), p=2, dim=1)\n",
        "      pckhyperILat = self.weight_layers[i](torch.nn.functional.normalize(torch.index_select(hyperILats[i], 0, uniqIids), p=2, dim=1))# @ self.weight_layers[i].weight\n",
        "      pckGnnILat = torch.nn.functional.normalize(torch.index_select(gnnILats[i], 0, uniqIids), p=2, dim=1)\n",
        "      uLoss = self.calcSSL(pckHyperULat, pckGnnULat)\n",
        "      iLoss = self.calcSSL(pckhyperILat, pckGnnILat)\n",
        "      sslloss += uLoss + iLoss\n",
        "\n",
        "    return preds, sslloss, self.Regularize([uEmbed0,iEmbed0,uhyper,ihyper])"
      ],
      "metadata": {
        "id": "C87ffNIJEEtG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HCCF.py"
      ],
      "metadata": {
        "id": "pQ8u5Pk0Ecvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-jekzb2stfm",
        "outputId": "b4345f21-bb82-4560-be92-57fe5d4804c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uEmbed0 torch.Size([29601, 32])\n",
            "iEmbed0 torch.Size([24734, 32])\n",
            "uhyper torch.Size([32, 128])\n",
            "ihyper torch.Size([32, 128])\n",
            "hyperULat_layers.0.fc1.W_fc torch.Size([128, 128])\n",
            "hyperULat_layers.0.fc2.W_fc torch.Size([128, 128])\n",
            "hyperULat_layers.0.fc3.W_fc torch.Size([128, 128])\n",
            "hyperULat_layers.1.fc1.W_fc torch.Size([128, 128])\n",
            "hyperULat_layers.1.fc2.W_fc torch.Size([128, 128])\n",
            "hyperULat_layers.1.fc3.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.0.fc1.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.0.fc2.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.0.fc3.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.1.fc1.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.1.fc2.W_fc torch.Size([128, 128])\n",
            "hyperILat_layers.1.fc3.W_fc torch.Size([128, 128])\n",
            "weight_layers.0.W torch.Size([32, 32])\n",
            "weight_layers.1.W torch.Size([32, 32])\n",
            "2022-09-07 13:10:54.467882: Model Prepared\n",
            "2022-09-07 13:11:04.471152: Epoch 0/100, Train: Loss = 36.5366, preLoss = 20.3680, sslLoss = 0.0016, regLoss = 0.0024  \n",
            "2022-09-07 13:11:42.321352: Epoch 0/100, Test: Recall = 0.0130, NDCG = 0.0114  \n",
            "\n",
            "2022-09-07 13:11:52.069960: Epoch 1/100, Train: Loss = 31.7214, preLoss = 15.5518, sslLoss = 0.0016, regLoss = 0.0027  \n",
            "\n",
            "2022-09-07 13:12:02.257881: Epoch 2/100, Train: Loss = 28.2516, preLoss = 12.1629, sslLoss = 0.0016, regLoss = 0.0029  \n",
            "\n",
            "2022-09-07 13:12:11.629059: Epoch 3/100, Train: Loss = 25.8237, preLoss = 9.8963, sslLoss = 0.0016, regLoss = 0.0031  \n",
            "2022-09-07 13:12:48.477745: Epoch 3/100, Test: Recall = 0.0286, NDCG = 0.0254  \n",
            "\n",
            "2022-09-07 13:12:57.954496: Epoch 4/100, Train: Loss = 23.9945, preLoss = 8.3852, sslLoss = 0.0016, regLoss = 0.0032  \n",
            "\n",
            "2022-09-07 13:13:08.181256: Epoch 5/100, Train: Loss = 22.4165, preLoss = 7.2727, sslLoss = 0.0015, regLoss = 0.0033  \n",
            "\n",
            "2022-09-07 13:13:17.368430: Epoch 6/100, Train: Loss = 21.0092, preLoss = 6.2965, sslLoss = 0.0015, regLoss = 0.0034  \n",
            "2022-09-07 13:13:54.004701: Epoch 6/100, Test: Recall = 0.0368, NDCG = 0.0320  \n",
            "\n",
            "2022-09-07 13:14:03.381040: Epoch 7/100, Train: Loss = 20.0945, preLoss = 5.7825, sslLoss = 0.0014, regLoss = 0.0034  \n",
            "\n",
            "2022-09-07 13:14:12.650723: Epoch 8/100, Train: Loss = 19.4016, preLoss = 5.3775, sslLoss = 0.0014, regLoss = 0.0035  \n",
            "\n",
            "2022-09-07 13:14:21.933689: Epoch 9/100, Train: Loss = 18.7660, preLoss = 4.9643, sslLoss = 0.0014, regLoss = 0.0036  \n",
            "2022-09-07 13:14:58.937956: Epoch 9/100, Test: Recall = 0.0427, NDCG = 0.0368  \n",
            "\n",
            "2022-09-07 13:15:08.401050: Epoch 10/100, Train: Loss = 18.1978, preLoss = 4.6633, sslLoss = 0.0014, regLoss = 0.0036  \n",
            "\n",
            "2022-09-07 13:15:17.914485: Epoch 11/100, Train: Loss = 17.8137, preLoss = 4.4477, sslLoss = 0.0013, regLoss = 0.0037  \n",
            "\n",
            "2022-09-07 13:15:27.208245: Epoch 12/100, Train: Loss = 17.4897, preLoss = 4.2600, sslLoss = 0.0013, regLoss = 0.0037  \n",
            "2022-09-07 13:16:05.437310: Epoch 12/100, Test: Recall = 0.0472, NDCG = 0.0401  \n",
            "\n",
            "2022-09-07 13:16:14.718069: Epoch 13/100, Train: Loss = 17.1647, preLoss = 4.1096, sslLoss = 0.0013, regLoss = 0.0037  \n",
            "\n",
            "2022-09-07 13:16:24.161756: Epoch 14/100, Train: Loss = 16.8764, preLoss = 3.9752, sslLoss = 0.0013, regLoss = 0.0038  \n",
            "\n",
            "2022-09-07 13:16:33.501778: Epoch 15/100, Train: Loss = 16.7987, preLoss = 3.9139, sslLoss = 0.0013, regLoss = 0.0038  \n",
            "2022-09-07 13:17:09.768431: Epoch 15/100, Test: Recall = 0.0486, NDCG = 0.0415  \n",
            "\n",
            "2022-09-07 13:17:19.189992: Epoch 16/100, Train: Loss = 16.4588, preLoss = 3.8045, sslLoss = 0.0013, regLoss = 0.0038  \n",
            "\n",
            "2022-09-07 13:17:28.501944: Epoch 17/100, Train: Loss = 16.3637, preLoss = 3.7469, sslLoss = 0.0013, regLoss = 0.0039  \n",
            "\n",
            "2022-09-07 13:17:37.688207: Epoch 18/100, Train: Loss = 16.0475, preLoss = 3.5703, sslLoss = 0.0012, regLoss = 0.0039  \n",
            "2022-09-07 13:18:15.494187: Epoch 18/100, Test: Recall = 0.0498, NDCG = 0.0428  \n",
            "\n",
            "2022-09-07 13:18:24.816040: Epoch 19/100, Train: Loss = 15.9359, preLoss = 3.5575, sslLoss = 0.0012, regLoss = 0.0039  \n",
            "\n",
            "2022-09-07 13:18:34.078769: Epoch 20/100, Train: Loss = 15.6863, preLoss = 3.4309, sslLoss = 0.0012, regLoss = 0.0040  \n",
            "\n",
            "2022-09-07 13:18:43.226136: Epoch 21/100, Train: Loss = 15.6000, preLoss = 3.4499, sslLoss = 0.0012, regLoss = 0.0040  \n",
            "2022-09-07 13:19:19.812325: Epoch 21/100, Test: Recall = 0.0517, NDCG = 0.0444  \n",
            "\n",
            "2022-09-07 13:19:29.103415: Epoch 22/100, Train: Loss = 15.3820, preLoss = 3.3248, sslLoss = 0.0012, regLoss = 0.0040  \n",
            "\n",
            "2022-09-07 13:19:38.342382: Epoch 23/100, Train: Loss = 15.3308, preLoss = 3.3545, sslLoss = 0.0012, regLoss = 0.0040  \n",
            "\n",
            "2022-09-07 13:19:47.696786: Epoch 24/100, Train: Loss = 15.1885, preLoss = 3.2514, sslLoss = 0.0012, regLoss = 0.0041  \n",
            "2022-09-07 13:20:24.632894: Epoch 24/100, Test: Recall = 0.0523, NDCG = 0.0449  \n",
            "\n",
            "2022-09-07 13:20:35.071295: Epoch 25/100, Train: Loss = 14.9721, preLoss = 3.2273, sslLoss = 0.0012, regLoss = 0.0041  \n",
            "\n",
            "2022-09-07 13:20:44.420545: Epoch 26/100, Train: Loss = 14.9010, preLoss = 3.1611, sslLoss = 0.0012, regLoss = 0.0041  \n",
            "\n",
            "2022-09-07 13:20:53.601800: Epoch 27/100, Train: Loss = 14.8946, preLoss = 3.1735, sslLoss = 0.0012, regLoss = 0.0041  \n",
            "2022-09-07 13:21:35.806121: Epoch 27/100, Test: Recall = 0.0533, NDCG = 0.0459  \n",
            "\n",
            "2022-09-07 13:21:45.324551: Epoch 28/100, Train: Loss = 14.7122, preLoss = 3.1113, sslLoss = 0.0012, regLoss = 0.0041  \n",
            "\n",
            "2022-09-07 13:21:54.621182: Epoch 29/100, Train: Loss = 14.5835, preLoss = 3.0782, sslLoss = 0.0012, regLoss = 0.0042  \n",
            "\n",
            "2022-09-07 13:22:03.780984: Epoch 30/100, Train: Loss = 14.5275, preLoss = 3.0934, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "2022-09-07 13:22:40.071749: Epoch 30/100, Test: Recall = 0.0546, NDCG = 0.0468  \n",
            "\n",
            "2022-09-07 13:22:49.333760: Epoch 31/100, Train: Loss = 14.4297, preLoss = 3.0120, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "\n",
            "2022-09-07 13:22:58.991331: Epoch 32/100, Train: Loss = 14.1815, preLoss = 2.9380, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "\n",
            "2022-09-07 13:23:08.710827: Epoch 33/100, Train: Loss = 14.1188, preLoss = 2.9223, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "2022-09-07 13:23:44.947570: Epoch 33/100, Test: Recall = 0.0549, NDCG = 0.0470  \n",
            "\n",
            "2022-09-07 13:23:54.128713: Epoch 34/100, Train: Loss = 14.2112, preLoss = 2.9885, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "\n",
            "2022-09-07 13:24:03.334436: Epoch 35/100, Train: Loss = 14.0411, preLoss = 2.8989, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "\n",
            "2022-09-07 13:24:12.536321: Epoch 36/100, Train: Loss = 13.8855, preLoss = 2.8400, sslLoss = 0.0011, regLoss = 0.0042  \n",
            "2022-09-07 13:24:48.990898: Epoch 36/100, Test: Recall = 0.0558, NDCG = 0.0479  \n",
            "\n",
            "2022-09-07 13:24:58.373452: Epoch 37/100, Train: Loss = 13.9191, preLoss = 2.8698, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:25:07.433055: Epoch 38/100, Train: Loss = 13.7765, preLoss = 2.8104, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:25:16.688774: Epoch 39/100, Train: Loss = 13.7079, preLoss = 2.7904, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "2022-09-07 13:25:54.218467: Epoch 39/100, Test: Recall = 0.0569, NDCG = 0.0486  \n",
            "\n",
            "2022-09-07 13:26:03.461659: Epoch 40/100, Train: Loss = 13.7460, preLoss = 2.8341, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:26:12.645150: Epoch 41/100, Train: Loss = 13.5972, preLoss = 2.7601, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:26:21.734223: Epoch 42/100, Train: Loss = 13.5875, preLoss = 2.7806, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "2022-09-07 13:26:58.255959: Epoch 42/100, Test: Recall = 0.0569, NDCG = 0.0486  \n",
            "\n",
            "2022-09-07 13:27:07.459832: Epoch 43/100, Train: Loss = 13.4880, preLoss = 2.7295, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:27:16.663323: Epoch 44/100, Train: Loss = 13.4375, preLoss = 2.6690, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:27:25.826690: Epoch 45/100, Train: Loss = 13.3696, preLoss = 2.6932, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "2022-09-07 13:28:03.443807: Epoch 45/100, Test: Recall = 0.0583, NDCG = 0.0495  \n",
            "\n",
            "2022-09-07 13:28:12.654395: Epoch 46/100, Train: Loss = 13.3124, preLoss = 2.6669, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:28:21.651676: Epoch 47/100, Train: Loss = 13.2182, preLoss = 2.6452, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "\n",
            "2022-09-07 13:28:30.776338: Epoch 48/100, Train: Loss = 13.2266, preLoss = 2.6641, sslLoss = 0.0011, regLoss = 0.0043  \n",
            "2022-09-07 13:29:07.091436: Epoch 48/100, Test: Recall = 0.0587, NDCG = 0.0499  \n",
            "\n",
            "2022-09-07 13:29:16.467444: Epoch 49/100, Train: Loss = 13.2318, preLoss = 2.6707, sslLoss = 0.0011, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:29:25.672778: Epoch 50/100, Train: Loss = 13.1464, preLoss = 2.6288, sslLoss = 0.0011, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:29:34.889148: Epoch 51/100, Train: Loss = 13.1244, preLoss = 2.6407, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:30:11.454115: Epoch 51/100, Test: Recall = 0.0592, NDCG = 0.0502  \n",
            "\n",
            "2022-09-07 13:30:20.759277: Epoch 52/100, Train: Loss = 12.9797, preLoss = 2.5908, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:30:29.900376: Epoch 53/100, Train: Loss = 13.0068, preLoss = 2.6005, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:30:40.251232: Epoch 54/100, Train: Loss = 12.9978, preLoss = 2.5718, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:31:16.835485: Epoch 54/100, Test: Recall = 0.0595, NDCG = 0.0506  \n",
            "\n",
            "2022-09-07 13:31:26.109473: Epoch 55/100, Train: Loss = 12.9816, preLoss = 2.5816, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:31:35.230739: Epoch 56/100, Train: Loss = 12.9397, preLoss = 2.5791, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:31:44.200274: Epoch 57/100, Train: Loss = 12.8581, preLoss = 2.5053, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:32:20.895211: Epoch 57/100, Test: Recall = 0.0598, NDCG = 0.0508  \n",
            "\n",
            "2022-09-07 13:32:30.189721: Epoch 58/100, Train: Loss = 12.9075, preLoss = 2.5296, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:32:39.315510: Epoch 59/100, Train: Loss = 12.8431, preLoss = 2.5430, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:32:48.495083: Epoch 60/100, Train: Loss = 12.8782, preLoss = 2.5768, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:33:26.344252: Epoch 60/100, Test: Recall = 0.0596, NDCG = 0.0508  \n",
            "\n",
            "2022-09-07 13:33:35.640375: Epoch 61/100, Train: Loss = 12.8360, preLoss = 2.5329, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:33:44.752485: Epoch 62/100, Train: Loss = 12.6802, preLoss = 2.4593, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:33:53.814458: Epoch 63/100, Train: Loss = 12.6230, preLoss = 2.4442, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:34:30.368404: Epoch 63/100, Test: Recall = 0.0605, NDCG = 0.0513  \n",
            "\n",
            "2022-09-07 13:34:39.643776: Epoch 64/100, Train: Loss = 12.7241, preLoss = 2.4989, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:34:48.747679: Epoch 65/100, Train: Loss = 12.6311, preLoss = 2.4652, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:34:58.098778: Epoch 66/100, Train: Loss = 12.7324, preLoss = 2.5024, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:35:34.706712: Epoch 66/100, Test: Recall = 0.0605, NDCG = 0.0515  \n",
            "\n",
            "2022-09-07 13:35:45.165457: Epoch 67/100, Train: Loss = 12.6044, preLoss = 2.4583, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:35:54.305045: Epoch 68/100, Train: Loss = 12.6242, preLoss = 2.4642, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:36:03.351643: Epoch 69/100, Train: Loss = 12.5700, preLoss = 2.4351, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:36:39.811821: Epoch 69/100, Test: Recall = 0.0608, NDCG = 0.0516  \n",
            "\n",
            "2022-09-07 13:36:49.057768: Epoch 70/100, Train: Loss = 12.5507, preLoss = 2.4472, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:36:58.285289: Epoch 71/100, Train: Loss = 12.6090, preLoss = 2.4590, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:37:07.464079: Epoch 72/100, Train: Loss = 12.5253, preLoss = 2.4407, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:37:43.949702: Epoch 72/100, Test: Recall = 0.0609, NDCG = 0.0519  \n",
            "\n",
            "2022-09-07 13:37:53.366854: Epoch 73/100, Train: Loss = 12.4813, preLoss = 2.4322, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:38:02.632097: Epoch 74/100, Train: Loss = 12.4417, preLoss = 2.4246, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:38:11.993404: Epoch 75/100, Train: Loss = 12.4553, preLoss = 2.4060, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:38:49.599545: Epoch 75/100, Test: Recall = 0.0618, NDCG = 0.0524  \n",
            "\n",
            "2022-09-07 13:38:58.886453: Epoch 76/100, Train: Loss = 12.4488, preLoss = 2.4110, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:39:07.966078: Epoch 77/100, Train: Loss = 12.4247, preLoss = 2.4358, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:39:17.101157: Epoch 78/100, Train: Loss = 12.4609, preLoss = 2.4094, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:39:53.933081: Epoch 78/100, Test: Recall = 0.0616, NDCG = 0.0522  \n",
            "\n",
            "2022-09-07 13:40:03.318265: Epoch 79/100, Train: Loss = 12.3609, preLoss = 2.3926, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:40:12.411105: Epoch 80/100, Train: Loss = 12.4763, preLoss = 2.4158, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:40:21.448008: Epoch 81/100, Train: Loss = 12.4066, preLoss = 2.3866, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:40:59.222555: Epoch 81/100, Test: Recall = 0.0617, NDCG = 0.0524  \n",
            "\n",
            "2022-09-07 13:41:08.520958: Epoch 82/100, Train: Loss = 12.4160, preLoss = 2.4201, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:41:17.787996: Epoch 83/100, Train: Loss = 12.3291, preLoss = 2.3495, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:41:26.930951: Epoch 84/100, Train: Loss = 12.3984, preLoss = 2.3825, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:42:03.651561: Epoch 84/100, Test: Recall = 0.0618, NDCG = 0.0525  \n",
            "\n",
            "2022-09-07 13:42:13.119025: Epoch 85/100, Train: Loss = 12.2988, preLoss = 2.3633, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:42:22.363857: Epoch 86/100, Train: Loss = 12.3781, preLoss = 2.4000, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:42:31.407136: Epoch 87/100, Train: Loss = 12.3636, preLoss = 2.3832, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:43:07.511804: Epoch 87/100, Test: Recall = 0.0622, NDCG = 0.0529  \n",
            "\n",
            "2022-09-07 13:43:16.764205: Epoch 88/100, Train: Loss = 12.3330, preLoss = 2.3855, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:43:26.497123: Epoch 89/100, Train: Loss = 12.3019, preLoss = 2.3829, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:43:36.303553: Epoch 90/100, Train: Loss = 12.3115, preLoss = 2.3712, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:44:13.133615: Epoch 90/100, Test: Recall = 0.0624, NDCG = 0.0529  \n",
            "\n",
            "2022-09-07 13:44:22.433332: Epoch 91/100, Train: Loss = 12.2657, preLoss = 2.3575, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:44:31.548447: Epoch 92/100, Train: Loss = 12.3083, preLoss = 2.3537, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:44:40.596396: Epoch 93/100, Train: Loss = 12.2561, preLoss = 2.3400, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:45:17.460731: Epoch 93/100, Test: Recall = 0.0625, NDCG = 0.0531  \n",
            "\n",
            "2022-09-07 13:45:26.914187: Epoch 94/100, Train: Loss = 12.2041, preLoss = 2.3092, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:45:36.199691: Epoch 95/100, Train: Loss = 12.2239, preLoss = 2.3166, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:45:45.472601: Epoch 96/100, Train: Loss = 12.2130, preLoss = 2.3204, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:46:23.328876: Epoch 96/100, Test: Recall = 0.0627, NDCG = 0.0531  \n",
            "\n",
            "2022-09-07 13:46:32.536575: Epoch 97/100, Train: Loss = 12.2837, preLoss = 2.3609, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:46:41.673017: Epoch 98/100, Train: Loss = 12.2184, preLoss = 2.3214, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "\n",
            "2022-09-07 13:46:50.821088: Epoch 99/100, Train: Loss = 12.2483, preLoss = 2.3366, sslLoss = 0.0010, regLoss = 0.0044  \n",
            "2022-09-07 13:47:27.919777: Epoch 99/100, Test: Recall = 0.0628, NDCG = 0.0533  \n",
            "\n",
            "2022-09-07 13:48:04.841636: Epoch 100/100, Test: Recall = 0.0628, NDCG = 0.0533  \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# from Model import HCCF\n",
        "# from DataHandler import DataHandler, negSamp, transToLsts, transpose\n",
        "# from Params import args\n",
        "# from TimeLogger import log\n",
        "\n",
        "torch.manual_seed(666)\n",
        "np.random.seed(666)\n",
        "\n",
        "class hccf():\n",
        "    def __init__(self,handler):\n",
        "        self.handler = handler\n",
        "        self.handler.LoadData()\n",
        "\n",
        "        adj = handler.trnMat\n",
        "        idx, data, shape = transToLsts(adj, norm=True)\n",
        "        self.adj_py = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).cuda()\n",
        "        idx, data, shape = transToLsts(transpose(adj), norm=True)\n",
        "        self.tpAdj_py = torch.sparse.FloatTensor(idx, data, shape).to(torch.float32).cuda()\n",
        "\n",
        "        self.curepoch = 0\n",
        "        self.metrics = dict()\n",
        "        mets = ['Loss', 'preLoss', 'Recall', 'NDCG']\n",
        "        for met in mets:\n",
        "          self.metrics['Train' + met] = list()\n",
        "          self.metrics['Test' + met] = list()\n",
        "\n",
        "    def preparemodel(self):\n",
        "        self.model = HCCF(self.adj_py, self.tpAdj_py).cuda()\n",
        "        self.opt = torch.optim.Adam(params = self.model.parameters(), lr=args.lr)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = self.opt, gamma=args.decayRate)\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                print(name, param.shape)\n",
        "\n",
        "    def sampleTrainBatch(self, batIds, labelMat):\n",
        "        temLabel = labelMat[batIds].toarray()\n",
        "        batch = len(batIds)\n",
        "        temlen = batch * 2 * args.sampNum\n",
        "        uLocs = [None] * temlen\n",
        "        iLocs = [None] * temlen\n",
        "        cur = 0\n",
        "        for i in range(batch):\n",
        "            posset = np.reshape(np.argwhere(temLabel[i]!=0), [-1])\n",
        "            sampNum = min(args.sampNum, len(posset))\n",
        "            if sampNum == 0:\n",
        "                poslocs = [np.random.choice(args.item)]\n",
        "                neglocs = [poslocs[0]]\n",
        "            else:\n",
        "                poslocs = np.random.choice(posset, sampNum)\n",
        "                neglocs = negSamp(temLabel[i], sampNum, args.item)\n",
        "            for j in range(sampNum):\n",
        "                posloc = poslocs[j]\n",
        "                negloc = neglocs[j]\n",
        "                uLocs[cur] = uLocs[cur+temlen//2] = batIds[i]\n",
        "                iLocs[cur] = posloc\n",
        "                iLocs[cur+temlen//2] = negloc\n",
        "                cur += 1\n",
        "        uLocsa = uLocs[:cur] + uLocs[temlen//2: temlen//2 + cur]\n",
        "        iLocsa = iLocs[:cur] + iLocs[temlen//2: temlen//2 + cur]\n",
        "        \n",
        "        return torch.Tensor(uLocsa).cuda(), torch.Tensor(iLocsa).cuda()\n",
        "\n",
        "    def trainEpoch(self):\n",
        "        args.actFunc = 'leakyRelu'\n",
        "\n",
        "        num = args.user\n",
        "        #randomly select args.trnNum users(10,000), from args.user(29,601 amazon), as input.\n",
        "        sfIds = np.random.permutation(args.user)[:args.trnNum]\n",
        "        epochLoss, epochPreLoss, epochsslloss, epochregloss = [0] * 4\n",
        "        num = len(sfIds)\n",
        "        steps = int(np.ceil(num / args.batch))\n",
        "        self.model.train()\n",
        "\n",
        "        for i in range(steps):\n",
        "            st = i * args.batch\n",
        "            ed = min((i+1) * args.batch, num)\n",
        "            batIds = sfIds[st: ed]\n",
        "\n",
        "            uLocs, iLocs = self.sampleTrainBatch(batIds, self.handler.trnMat)\n",
        "            \n",
        "            preds, sslloss, regularize = self.model(uLocs.int(),iLocs.int())\n",
        "            sampNum = uLocs.shape[0] // 2\n",
        "            posPred = preds[:sampNum]\n",
        "            negPred = preds[sampNum:sampNum * 2]\n",
        "            preLoss = torch.sum(torch.maximum(torch.Tensor([0.0]).to(args.device), 1.0 - (posPred - negPred))) / args.batch\n",
        "            sslloss = args.ssl_reg * sslloss\n",
        "            regLoss = args.reg * regularize\n",
        "\n",
        "            loss = preLoss + regLoss + sslloss\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if i % args.decay_step == 0:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            epochLoss += loss\n",
        "            epochPreLoss += preLoss\n",
        "            epochregloss += args.reg * regularize\n",
        "            epochsslloss += args.ssl_reg * sslloss\n",
        "            #log('Step %d/%d: loss = %.2f, regLoss = %.2f         ' % (i, steps, loss, regLoss), save=False, oneline=False)\n",
        "\n",
        "        ret = dict()\n",
        "        ret['Loss'] = epochLoss / steps\n",
        "        ret['preLoss'] = epochPreLoss / steps\n",
        "        ret['sslLoss'] = epochsslloss / steps\n",
        "        ret['regLoss'] = epochregloss / steps\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def testEpoch(self):\n",
        "      self.model.eval()\n",
        "      with torch.no_grad():\n",
        "        epochRecall, epochNdcg = [0] * 2\n",
        "        ids = self.handler.tstUsrs\n",
        "        num = len(ids)\n",
        "        tstBat = args.batch\n",
        "        steps = int(np.ceil(num / tstBat))\n",
        "        tstNum = 0\n",
        "        ulat, ilat = self.model.forward_test()\n",
        "        for i in range(steps):\n",
        "            st = i * tstBat\n",
        "            ed = min((i+1) * tstBat, num)\n",
        "            batIds = ids[st: ed]\n",
        "            trnPosMask = self.handler.trnMat[batIds].toarray()\n",
        "            toplocs = self.tstPred(batIds, trnPosMask, ulat, ilat)\n",
        "            recall, ndcg = self.calcRes(toplocs, self.handler.tstLocs, batIds)\n",
        "            epochRecall += recall\n",
        "            epochNdcg += ndcg\n",
        "            #log('Steps %d/%d: recall = %.2f, ndcg = %.2f          ' % (i, steps, recall, ndcg), save=False, oneline=False)\n",
        "        ret = dict()\n",
        "        ret['Recall'] = epochRecall / num\n",
        "        ret['NDCG'] = epochNdcg / num\n",
        "      return ret\n",
        "\n",
        "    def tstPred(self, batIds, trnPosMask, ulat, ilat):\n",
        "      pckUlat = torch.index_select(ulat, 0, torch.Tensor(batIds).int().to(args.device))\n",
        "      allPreds = pckUlat @ torch.transpose(ilat, 0, 1)\n",
        "      allPreds = allPreds.cpu().detach().numpy() * (1 - trnPosMask) - trnPosMask * 1e8\n",
        "      vals, locs = torch.topk(torch.tensor(allPreds), args.shoot)\n",
        "      return locs\n",
        "\n",
        "    def calcRes(self, topLocs, tstLocs, batIds):\n",
        "        assert topLocs.shape[0] == len(batIds)\n",
        "        allRecall = allNdcg = 0\n",
        "        recallBig = 0\n",
        "        ndcgBig =0\n",
        "        for i in range(len(batIds)):\n",
        "            temTopLocs = list(topLocs[i])\n",
        "            temTstLocs = tstLocs[batIds[i]]\n",
        "            tstNum = len(temTstLocs)\n",
        "            maxDcg = np.sum([np.reciprocal(np.log2(loc + 2)) for loc in range(min(tstNum, args.shoot))])\n",
        "            recall = dcg = 0\n",
        "            for val in temTstLocs:\n",
        "                if val in temTopLocs:\n",
        "                    recall += 1\n",
        "                    dcg += np.reciprocal(np.log2(temTopLocs.index(val) + 2))\n",
        "            recall = recall / tstNum\n",
        "            ndcg = dcg / maxDcg\n",
        "            allRecall += recall\n",
        "            allNdcg += ndcg\n",
        "        return allRecall, allNdcg\n",
        "\n",
        "    def loadModel(self, loadPath):\n",
        "        loadPath = loadPath\n",
        "        checkpoint = torch.load(loadPath)\n",
        "        self.model = checkpoint['model']\n",
        "        self.curepoch = checkpoint['epoch']+1\n",
        "        self.metrics = checkpoint['metrics']\n",
        "\n",
        "    def saveHistory(self):\n",
        "\n",
        "        savePath = r'./Model/' + args.data  + r'.pth'\n",
        "        params = {\n",
        "            'epoch' : self.curepoch,\n",
        "            'model' : self.model,\n",
        "            'metrics' : self.metrics,\n",
        "        }\n",
        "        torch.save(params, savePath)\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        self.preparemodel()\n",
        "        log('Model Prepared')\n",
        "        if args.load_model != None:\n",
        "            self.loadModel(args.load_model)\n",
        "            stloc = self.curepoch\n",
        "        else:\n",
        "            stloc = 0\n",
        "\n",
        "        for ep in range(stloc, args.epoch):\n",
        "            test = (ep % args.tstEpoch == 0)\n",
        "            reses = self.trainEpoch()\n",
        "            #print(self.model.hyperULat_layers[0].fc1.W_fc.weight)\n",
        "            log(self.makePrint('Train', ep, reses, test))\n",
        "            if test:\n",
        "                reses = self.testEpoch()\n",
        "                log(self.makePrint('Test', ep, reses, test))\n",
        "            if ep % args.tstEpoch == 0:\n",
        "                self.saveHistory()\n",
        "            print()\n",
        "            self.curepoch = ep\n",
        "        reses = self.testEpoch()\n",
        "        log(self.makePrint('Test', args.epoch, reses, True))\n",
        "        self.saveHistory()\n",
        "\n",
        "    def makePrint(self, name, ep, reses, save):\n",
        "      ret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n",
        "      for metric in reses:\n",
        "            val = reses[metric]\n",
        "            ret += '%s = %.4f, ' % (metric, val)\n",
        "            tem = name + metric\n",
        "            if save and tem in self.metrics:\n",
        "                self.metrics[tem].append(val)\n",
        "      ret = ret[:-2] + '  '\n",
        "      return ret\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    handler = DataHandler()\n",
        "    handler.LoadData()\n",
        "    model=hccf(handler)\n",
        "    model.run()"
      ]
    }
  ]
}